# HAN (Heterogeneous Attention Network)

## M√¥ t·∫£

HAN implementation d·ª±a tr√™n paper "Heterogeneous Graph Attention Network" (Wang et al., WWW 2019).

HAN l√† state-of-the-art model cho **heterogeneous graphs** v·ªõi **hierarchical attention mechanism**.

## Architecture

### HAN Algorithm

HAN c√≥ 2 levels c·ªßa attention:

#### 1. Node-Level Attention
Aggregation trong m·ªói meta-path:
```
Œ±_ij = attention(h_i, h_j) in meta-path Œ¶
h_i^Œ¶ = Œ£_j Œ±_ij h_j
```

#### 2. Semantic-Level Attention
AggregHamilton across meta-paths:
```
Œ≤_Œ¶ = attention(h_i^Œ¶, q)
z_i = Œ£_Œ¶ Œ≤_Œ¶ h_i^Œ¶
```

Trong ƒë√≥:
- **Œ¶**: Meta-path (e.g., Transaction-User-Transaction)
- **Œ±_ij**: Node-level attention weight
- **Œ≤_Œ¶**: Semantic-level attention weight (meta-path importance)
- **q**: Semantic attention query vector (learnable)

### Meta-Paths cho Fraud Detection

```
1. TUT: Transaction ‚Üí User ‚Üí Transaction
   Captures: "Transactions by same user"
   
2. TDT: Transaction ‚Üí Device ‚Üí Transaction
   Captures: "Transactions from same device"
   
3. TUDT: Transaction ‚Üí User ‚Üí Device ‚Üí Transaction
   Captures: "Users sharing devices"
```

### Hierarchical Attention

```
                    [Final Embedding]
                           ‚Üë
                  Semantic Attention
                    (Œ≤_TUT, Œ≤_TDT)
                           ‚Üë
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚Üì                         ‚Üì
      [TUT Embedding]           [TDT Embedding]
              ‚Üë                         ‚Üë
       Node Attention            Node Attention
         (Œ±_ij in TUT)            (Œ±_ij in TDT)
              ‚Üë                         ‚Üë
      [Transaction Features]    [Transaction Features]
```

## 3 Variants ƒë√£ implement

### 1. **HAN (PyG's HANConv)**
- Uses PyTorch Geometric's HANConv layer
- Automatic meta-path handling
- Best for standard use cases
- Easiest to use

### 2. **CustomHAN**
- Explicit meta-path computation
- Custom GAT layers cho m·ªói meta-path
- Full control over architecture
- Best for research v√† customization

### 3. **SimpleHAN**
- Simplified version v·ªõi 2 meta-paths (TUT, TDT)
- Easy to understand v√† modify
- Learnable semantic attention weights
- Best for getting started

## ∆Øu ƒëi·ªÉm c·ªßa HAN

1. **Hierarchical Attention**: 2 levels attention cho expressive power
2. **Meta-path based**: Capture high-order relationships
3. **Interpretable**: Semantic attention shows meta-path importance
4. **State-of-the-art**: Best performance tr√™n heterogeneous graphs
5. **Flexible**: Support multiple meta-paths

## So s√°nh v·ªõi RGCN

| Feature | RGCN | HAN |
|---------|------|-----|
| **Attention** | ‚ùå No | ‚úÖ Hierarchical |
| **Relations** | Fixed weights W_r | Learned attention Œ±, Œ≤ |
| **Meta-paths** | Implicit | Explicit |
| **Interpretability** | Low | **High** (semantic attention) |
| **Complexity** | Medium | High |
| **Parameters** | O(B¬∑d¬≤) | O(P¬∑H¬∑d¬≤) |
| **Performance** | Good | **Better** |
| **For Fraud** | Good | **Best** |

Trong ƒë√≥:
- B: Number of bases
- P: Number of meta-paths
- H: Number of attention heads
- d: Hidden dimension

## S·ª≠ d·ª•ng

### Training

```python
# Simple HAN (recommended to start)
python HAN/train.py

# With custom config
from HAN.train import train_han

config = {
    'model_type': 'simple',   # 'simple', 'han', or 'custom'
    'hidden_channels': 64,
    'num_heads': 8,
    'dropout': 0.6,
    'epochs': 300,
    'lr': 0.005,
}

model, test_metrics, tracker = train_han(config)

# Try other variants
config['model_type'] = 'han'      # PyG's HANConv
config['model_type'] = 'custom'   # Custom implementation
```

### Inference

```python
from HAN.model import create_han_model, extract_meta_paths

# Prepare data
hetero_data = prepare_hetero_data_with_features(df, feature_cols)
tut_edge_index, tdt_edge_index = extract_meta_paths(hetero_data)

# Create model
model = create_han_model(
    model_type='simple',
    num_nodes_dict={
        'transaction': n_trans,
        'user': n_user,
        'device': n_device
    },
    in_channels=feature_dim,
    hidden_channels=64,
    num_heads=8
)

# Load checkpoint
checkpoint = torch.load('checkpoints/HAN/HAN_best.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# Predict
x_dict = {'transaction': hetero_data['transaction'].x}
pred_labels, pred_probs = model.predict(x_dict, tut_edge_index, tdt_edge_index)

# Get semantic attention weights
weights = model.get_semantic_weights()
print(f"TUT importance: {weights[0]:.3f}")
print(f"TDT importance: {weights[1]:.3f}")
```

### Analyzing Meta-Path Importance

```python
# SimpleHAN
weights = model.get_semantic_weights()
print(f"Transaction-User-Transaction: {weights[0]:.4f}")
print(f"Transaction-Device-Transaction: {weights[1]:.4f}")

# CustomHAN
weights = model.get_semantic_attention_weights()
for i, (name, w) in enumerate(zip(['TUT', 'TDT'], weights)):
    print(f"{name}: {w:.4f}")
```

## Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| hidden_channels | 64 | Hidden dimension |
| num_heads | 8 | Number of attention heads |
| dropout | 0.6 | Dropout rate (high cho attention) |
| lr | 0.005 | Learning rate (lower than GCN) |
| weight_decay | 5e-4 | L2 regularization |
| patience | 40 | Early stopping patience (HAN needs longer) |
| epochs | 300 | Maximum epochs |

### Attention Heads

**Trade-off:**
- **Few heads** (4):
  - Faster training
  - Lower memory
  - May miss relationships
  
- **Many heads** (8-16):
  - Better attention diversity
  - Capture more patterns
  - Higher memory usage
  - **Recommended: 8**

### Dropout

HAN typically uses **higher dropout** than other models:
- Standard models: 0.3-0.5
- HAN: 0.5-0.6
- Reason: Attention mechanism prone to overfitting

## Tips & Best Practices

1. **Model selection**:
   - **SimpleHAN**: Start here, easiest to understand
   - **HAN (PyG)**: Production use, well-tested
   - **CustomHAN**: Research, need full control

2. **Meta-paths**:
   - Start v·ªõi 2-3 meta-paths
   - Too many ‚Üí overfitting, slower training
   - Analyze semantic attention ƒë·ªÉ prune weak meta-paths

3. **Attention heads**:
   - 8 heads recommended
   - 4 heads cho quick testing
   - 16 heads n·∫øu c√≥ sufficient data

4. **Training**:
   - HAN converges slower ‚Üí use patience=40+
   - Lower learning rate (0.005 vs 0.01)
   - High dropout (0.6)

5. **Meta-path extraction**:
   - Can be slow cho large graphs
   - Cache extracted meta-paths
   - Use sparse representation

6. **Interpretability**:
   - Always analyze semantic attention weights
   - Identify most important meta-paths
   - Can prune unimportant meta-paths

## K·∫øt qu·∫£ mong ƒë·ª£i

| Model | AUC-ROC | F1-Score | Training Time (10K samples) | Parameters |
|-------|---------|----------|---------------------------|------------|
| SimpleHAN | 0.86-0.93 | 0.64-0.79 | ~18 ph√∫t | ~450K |
| HAN (PyG) | 0.87-0.94 | 0.65-0.80 | ~20 ph√∫t | ~500K |
| CustomHAN | 0.86-0.93 | 0.64-0.79 | ~22 ph√∫t | ~550K |

### Semantic Attention Analysis

Typical semantic attention weights:
```
Transaction-User-Transaction (TUT): 0.45-0.65
Transaction-Device-Transaction (TDT): 0.35-0.55
```

**Interpretation:**
- **TUT > TDT**: User patterns more important
- **TDT > TUT**: Device patterns more important
- **Balanced**: Both equally important

## Performance vs Other Models

| Model | AUC-ROC | Captures Relations | Attention | Interpretability |
|-------|---------|-------------------|-----------|------------------|
| GCN | 0.80-0.88 | ‚ùå Limited | ‚ùå No | Low |
| GAT | 0.82-0.90 | ‚ùå Limited | ‚úÖ Node-level | Medium |
| GraphSAGE | 0.81-0.89 | ‚ùå Limited | ‚ùå No | Low |
| RGCN | 0.84-0.92 | ‚úÖ Full | ‚ùå No | Medium |
| **HAN** | **0.86-0.94** | ‚úÖ **Full** | ‚úÖ **Hierarchical** | **High** |

HAN **consistently outperforms** all other models v√¨:
- Hierarchical attention captures complex patterns
- Meta-paths capture high-order relationships
- Semantic attention learns meta-path importance
- Best modeling c·ªßa heterogeneous fraud data

## When to use HAN?

‚úÖ **Use HAN when:**
- Need best possible performance
- Want interpretability (meta-path importance)
- Have heterogeneous graph v·ªõi multiple relations
- Can afford longer training time
- Fraud detection v·ªõi complex patterns

‚ùå **Consider alternatives when:**
- Need fast inference ‚Üí Use GraphSAGE
- Simple graph ‚Üí Use GCN/GAT
- Limited computational resources ‚Üí Use RGCN
- Homogeneous graph ‚Üí Use GAT

## Model Evolution

```
GCN (Homogeneous, No Attention)
    ‚Üì
GAT (Homogeneous, Node-level Attention)
    ‚Üì
RGCN (Heterogeneous, No Attention)
    ‚Üì
HAN (Heterogeneous, Hierarchical Attention) ‚Üê Best
```

## Research Insights

### Meta-Path Importance

From experiments:
1. **TUT** (Transaction-User-Transaction):
   - Captures: Users with multiple fraudulent transactions
   - Weight: Usually 0.5-0.6
   - Most important for fraud detection

2. **TDT** (Transaction-Device-Transaction):
   - Captures: Devices used for multiple transactions
   - Weight: Usually 0.4-0.5
   - Important for device-based fraud

### Why HAN works best?

1. **Hierarchical attention**: 
   - Node-level: Which neighbors matter
   - Semantic-level: Which relationships matter

2. **Meta-paths**:
   - Capture high-order patterns
   - "User A and User B share device" ‚Üí suspicious

3. **Interpretability**:
   - See which meta-paths are important
   - Explain model decisions

## Comparison Summary

| Aspect | Homogeneous (GCN/GAT/GraphSAGE) | RGCN | HAN |
|--------|--------------------------------|------|-----|
| **Graph Type** | Single node type | Multi node types | Multi node types |
| **Edge Handling** | Same weights | Relation-specific weights | Attention-based |
| **High-order** | ‚ùå No | ‚ö†Ô∏è Limited | ‚úÖ Meta-paths |
| **Attention** | ‚ö†Ô∏è Node-level only (GAT) | ‚ùå No | ‚úÖ Hierarchical |
| **Interpretability** | Low-Medium | Medium | **High** |
| **Performance** | Good | Better | **Best** |
| **Training Time** | Fast | Medium | Slow |
| **Best For** | Simple graphs | Relational data | **Fraud detection** |

## References

1. Wang et al. (2019). "Heterogeneous Graph Attention Network", WWW 2019
2. Schlichtkrull et al. (2018). "Modeling Relational Data with Graph Convolutional Networks", ESWC 2018
3. Veliƒçkoviƒá et al. (2018). "Graph Attention Networks", ICLR 2018
4. Kipf & Welling (2017). "Semi-Supervised Classification with Graph Convolutional Networks", ICLR 2017

## Conclusion

**HAN is the BEST model cho fraud detection** khi:
- C√≥ heterogeneous data (user, device, transaction)
- C·∫ßn high performance
- C·∫ßn interpretability
- Can afford training time

**Expected improvement over baselines:**
- vs GCN: +6-8% AUC-ROC
- vs GAT: +4-6% AUC-ROC
- vs GraphSAGE: +5-7% AUC-ROC
- vs RGCN: +2-4% AUC-ROC

üéØ **HAN l√† model recommend cho production fraud detection system!**
